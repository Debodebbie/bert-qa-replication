C:\DEV_Tools\python\python.exe C:/Users/Prophet.DESKTOP-UUFA83J/PycharmProjects/bert-qa-replication/probing_task_v4_semEval_destilBERT.py
Using custom data configuration default
Reusing dataset sem_eval2010_task8 (C:\Users\Prophet.DESKTOP-UUFA83J\.cache\huggingface\datasets\sem_eval2010_task8\default\1.0.0\8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416)
Using custom data configuration default
{'sentence': 'The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.', 'relation': 3}
load model
Reusing dataset sem_eval2010_task8 (C:\Users\Prophet.DESKTOP-UUFA83J\.cache\huggingface\datasets\sem_eval2010_task8\default\1.0.0\8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416)
model loaded
prepare_dataset
8000
2717
load classifier from training epoch 1
evaluation started
[   24] loss:, [2.952, 2.871, 2.77, 2.789, 2.719, 2.734, 2.75, 2.744, 2.749, 2.684, 2.677, 2.68, 2.674, 0.0]
[   49] loss:, [2.906, 2.816, 2.731, 2.744, 2.669, 2.672, 2.669, 2.666, 2.669, 2.666, 2.672, 2.679, 2.654, 0.0]
[   74] loss:, [2.87, 2.786, 2.692, 2.701, 2.613, 2.615, 2.603, 2.598, 2.6, 2.587, 2.601, 2.611, 2.596, 0.0]
[   99] loss:, [2.844, 2.798, 2.716, 2.718, 2.623, 2.626, 2.611, 2.618, 2.622, 2.595, 2.613, 2.617, 2.616, 0.0]
[  124] loss:, [2.824, 2.778, 2.7, 2.697, 2.602, 2.602, 2.583, 2.585, 2.588, 2.563, 2.576, 2.579, 2.584, 0.0]
[  149] loss:, [2.836, 2.791, 2.709, 2.705, 2.623, 2.625, 2.601, 2.597, 2.597, 2.568, 2.578, 2.584, 2.587, 0.0]
[  174] loss:, [2.831, 2.786, 2.709, 2.707, 2.624, 2.628, 2.604, 2.594, 2.599, 2.567, 2.579, 2.581, 2.583, 0.0]
[  199] loss:, [2.844, 2.79, 2.713, 2.709, 2.629, 2.633, 2.613, 2.594, 2.6, 2.567, 2.584, 2.591, 2.586, 0.0]
[  224] loss:, [2.846, 2.793, 2.717, 2.715, 2.635, 2.639, 2.621, 2.597, 2.604, 2.57, 2.59, 2.6, 2.596, 0.0]
[  249] loss:, [2.84, 2.787, 2.713, 2.712, 2.635, 2.638, 2.618, 2.591, 2.599, 2.568, 2.587, 2.599, 2.596, 0.0]
[  274] loss:, [2.837, 2.784, 2.71, 2.711, 2.631, 2.631, 2.616, 2.588, 2.598, 2.565, 2.587, 2.599, 2.596, 0.0]
[  299] loss:, [2.83, 2.781, 2.706, 2.706, 2.626, 2.623, 2.61, 2.592, 2.601, 2.565, 2.583, 2.597, 2.596, 0.0]
[  324] loss:, [2.835, 2.785, 2.705, 2.704, 2.621, 2.617, 2.606, 2.586, 2.594, 2.561, 2.58, 2.592, 2.59, 0.0]
[  349] loss:, [2.834, 2.785, 2.703, 2.702, 2.619, 2.616, 2.604, 2.588, 2.593, 2.556, 2.575, 2.589, 2.588, 0.0]
[  374] loss:, [2.834, 2.785, 2.701, 2.698, 2.615, 2.611, 2.598, 2.581, 2.587, 2.549, 2.569, 2.583, 2.579, 0.0]
[  399] loss:, [2.826, 2.774, 2.691, 2.688, 2.607, 2.603, 2.591, 2.574, 2.581, 2.543, 2.563, 2.575, 2.57, 0.0]
[  424] loss:, [2.822, 2.77, 2.693, 2.693, 2.612, 2.608, 2.596, 2.578, 2.587, 2.547, 2.566, 2.577, 2.574, 0.0]
[  449] loss:, [2.818, 2.768, 2.689, 2.688, 2.607, 2.601, 2.59, 2.573, 2.584, 2.542, 2.561, 2.573, 2.569, 0.0]
[  474] loss:, [2.814, 2.762, 2.685, 2.685, 2.605, 2.601, 2.588, 2.572, 2.583, 2.541, 2.558, 2.569, 2.568, 0.0]
[  499] loss:, [2.818, 2.763, 2.683, 2.682, 2.599, 2.595, 2.583, 2.566, 2.576, 2.536, 2.556, 2.567, 2.561, 0.0]
[  524] loss:, [2.817, 2.761, 2.681, 2.68, 2.597, 2.593, 2.581, 2.561, 2.572, 2.531, 2.551, 2.562, 2.555, 0.0]
[  549] loss:, [2.819, 2.762, 2.683, 2.682, 2.597, 2.593, 2.581, 2.564, 2.573, 2.533, 2.551, 2.563, 2.556, 0.0]
[  574] loss:, [2.816, 2.758, 2.68, 2.68, 2.596, 2.591, 2.577, 2.559, 2.569, 2.528, 2.548, 2.56, 2.553, 0.0]
[  599] loss:, [2.814, 2.757, 2.679, 2.678, 2.593, 2.587, 2.574, 2.555, 2.564, 2.526, 2.546, 2.558, 2.549, 0.0]
[  624] loss:, [2.809, 2.754, 2.678, 2.677, 2.591, 2.585, 2.574, 2.555, 2.566, 2.526, 2.547, 2.558, 2.551, 0.0]
[  649] loss:, [2.808, 2.751, 2.676, 2.675, 2.591, 2.586, 2.575, 2.556, 2.566, 2.525, 2.546, 2.558, 2.552, 0.0]
[  674] loss:, [2.809, 2.754, 2.677, 2.676, 2.593, 2.589, 2.577, 2.555, 2.564, 2.526, 2.546, 2.557, 2.553, 0.0]
[  699] loss:, [2.807, 2.753, 2.678, 2.676, 2.592, 2.587, 2.575, 2.554, 2.563, 2.526, 2.546, 2.555, 2.552, 0.0]
[  724] loss:, [2.807, 2.754, 2.679, 2.677, 2.593, 2.589, 2.576, 2.554, 2.564, 2.524, 2.543, 2.553, 2.552, 0.0]
[  749] loss:, [2.807, 2.754, 2.677, 2.675, 2.592, 2.587, 2.575, 2.554, 2.564, 2.524, 2.542, 2.553, 2.55, 0.0]
[  774] loss:, [2.809, 2.753, 2.677, 2.675, 2.593, 2.59, 2.576, 2.555, 2.566, 2.524, 2.546, 2.556, 2.554, 0.0]
[  799] loss:, [2.809, 2.752, 2.674, 2.672, 2.59, 2.585, 2.571, 2.551, 2.561, 2.521, 2.542, 2.552, 2.55, 0.0]
[  824] loss:, [2.811, 2.754, 2.677, 2.675, 2.594, 2.589, 2.575, 2.555, 2.565, 2.525, 2.544, 2.555, 2.553, 0.0]
[  849] loss:, [2.81, 2.753, 2.675, 2.673, 2.592, 2.587, 2.572, 2.552, 2.563, 2.522, 2.541, 2.552, 2.55, 0.0]
[  874] loss:, [2.809, 2.75, 2.673, 2.671, 2.591, 2.586, 2.571, 2.553, 2.562, 2.522, 2.54, 2.551, 2.55, 0.0]
[  899] loss:, [2.811, 2.752, 2.674, 2.673, 2.593, 2.588, 2.574, 2.556, 2.565, 2.525, 2.544, 2.555, 2.554, 0.0]
[  924] loss:, [2.814, 2.756, 2.677, 2.676, 2.595, 2.591, 2.577, 2.558, 2.566, 2.527, 2.546, 2.556, 2.556, 0.0]
[  949] loss:, [2.812, 2.754, 2.677, 2.677, 2.596, 2.592, 2.577, 2.558, 2.566, 2.527, 2.546, 2.556, 2.556, 0.0]
[  974] loss:, [2.811, 2.753, 2.676, 2.675, 2.595, 2.591, 2.576, 2.556, 2.565, 2.525, 2.545, 2.554, 2.554, 0.0]
[  999] loss:, [2.814, 2.756, 2.677, 2.677, 2.597, 2.594, 2.579, 2.558, 2.566, 2.525, 2.545, 2.554, 2.554, 0.0]
[ 1024] loss:, [2.814, 2.756, 2.676, 2.676, 2.596, 2.594, 2.579, 2.557, 2.565, 2.525, 2.544, 2.553, 2.554, 0.0]
[ 1049] loss:, [2.818, 2.759, 2.678, 2.679, 2.599, 2.597, 2.582, 2.559, 2.568, 2.527, 2.547, 2.556, 2.558, 0.0]
[ 1074] loss:, [2.817, 2.758, 2.677, 2.677, 2.596, 2.594, 2.578, 2.555, 2.563, 2.524, 2.544, 2.553, 2.555, 0.0]
[ 1099] loss:, [2.816, 2.757, 2.676, 2.676, 2.596, 2.593, 2.578, 2.553, 2.562, 2.523, 2.542, 2.551, 2.553, 0.0]
[ 1124] loss:, [2.816, 2.757, 2.675, 2.675, 2.595, 2.592, 2.577, 2.552, 2.561, 2.521, 2.54, 2.549, 2.553, 0.0]
[ 1149] loss:, [2.815, 2.755, 2.673, 2.673, 2.592, 2.589, 2.574, 2.55, 2.558, 2.518, 2.537, 2.546, 2.55, 0.0]
[ 1174] loss:, [2.815, 2.755, 2.672, 2.671, 2.591, 2.588, 2.572, 2.548, 2.556, 2.517, 2.536, 2.544, 2.549, 0.0]
[ 1199] loss:, [2.815, 2.756, 2.673, 2.672, 2.592, 2.589, 2.574, 2.55, 2.557, 2.519, 2.537, 2.545, 2.55, 0.0]
[ 1224] loss:, [2.815, 2.755, 2.673, 2.672, 2.591, 2.588, 2.572, 2.549, 2.556, 2.518, 2.536, 2.544, 2.549, 0.0]
[ 1249] loss:, [2.813, 2.752, 2.671, 2.67, 2.59, 2.588, 2.572, 2.548, 2.556, 2.518, 2.536, 2.544, 2.549, 0.0]
[ 1274] loss:, [2.813, 2.753, 2.672, 2.671, 2.592, 2.588, 2.573, 2.55, 2.557, 2.519, 2.537, 2.545, 2.55, 0.0]
[ 1299] loss:, [2.813, 2.754, 2.674, 2.673, 2.593, 2.589, 2.573, 2.551, 2.558, 2.52, 2.537, 2.546, 2.55, 0.0]
[ 1324] loss:, [2.812, 2.754, 2.674, 2.673, 2.593, 2.59, 2.574, 2.552, 2.559, 2.521, 2.538, 2.546, 2.551, 0.0]
[ 1349] loss:, [2.814, 2.756, 2.676, 2.674, 2.595, 2.591, 2.576, 2.553, 2.561, 2.521, 2.538, 2.547, 2.552, 0.0]
training finished

Process finished with exit code 0
