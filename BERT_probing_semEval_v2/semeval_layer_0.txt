C:\DEV_Tools\python\python.exe C:/Users/Prophet.DESKTOP-UUFA83J/PycharmProjects/bert-qa-replication/probing_task_v4_semEval_destilBERT.py
Using custom data configuration default
Reusing dataset sem_eval2010_task8 (C:\Users\Prophet.DESKTOP-UUFA83J\.cache\huggingface\datasets\sem_eval2010_task8\default\1.0.0\8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416)
Using custom data configuration default
Reusing dataset sem_eval2010_task8 (C:\Users\Prophet.DESKTOP-UUFA83J\.cache\huggingface\datasets\sem_eval2010_task8\default\1.0.0\8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416)
{'sentence': 'The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.', 'relation': 3}
load model
model loaded
prepare_dataset
8000
2717
load classifier from training epoch 0
evaluation started
[   24] loss:, [2.96, 2.934, 2.85, 2.867, 2.807, 2.817, 2.82, 2.758, 2.771, 2.755, 2.707, 2.716, 2.769, 0.0]
[   49] loss:, [2.913, 2.872, 2.795, 2.805, 2.737, 2.738, 2.73, 2.672, 2.697, 2.714, 2.681, 2.692, 2.734, 0.0]
[   74] loss:, [2.875, 2.835, 2.756, 2.761, 2.679, 2.682, 2.668, 2.616, 2.634, 2.64, 2.616, 2.623, 2.668, 0.0]
[   99] loss:, [2.849, 2.84, 2.773, 2.773, 2.684, 2.684, 2.676, 2.645, 2.66, 2.65, 2.631, 2.632, 2.682, 0.0]
[  124] loss:, [2.829, 2.815, 2.753, 2.751, 2.66, 2.66, 2.648, 2.614, 2.628, 2.618, 2.595, 2.599, 2.648, 0.0]
[  149] loss:, [2.84, 2.826, 2.76, 2.759, 2.676, 2.675, 2.659, 2.623, 2.636, 2.62, 2.601, 2.606, 2.646, 0.0]
[  174] loss:, [2.835, 2.82, 2.758, 2.758, 2.675, 2.677, 2.659, 2.623, 2.637, 2.624, 2.604, 2.607, 2.642, 0.0]
[  199] loss:, [2.847, 2.826, 2.764, 2.761, 2.678, 2.68, 2.664, 2.631, 2.647, 2.631, 2.61, 2.617, 2.648, 0.0]
[  224] loss:, [2.85, 2.829, 2.769, 2.768, 2.686, 2.687, 2.672, 2.638, 2.658, 2.636, 2.619, 2.629, 2.659, 0.0]
[  249] loss:, [2.843, 2.824, 2.764, 2.765, 2.686, 2.686, 2.671, 2.634, 2.652, 2.631, 2.617, 2.627, 2.66, 0.0]
[  274] loss:, [2.84, 2.82, 2.761, 2.763, 2.682, 2.68, 2.668, 2.632, 2.654, 2.628, 2.616, 2.627, 2.659, 0.0]
[  299] loss:, [2.833, 2.816, 2.758, 2.759, 2.676, 2.671, 2.661, 2.635, 2.654, 2.627, 2.613, 2.625, 2.657, 0.0]
[  324] loss:, [2.838, 2.82, 2.759, 2.76, 2.673, 2.668, 2.659, 2.633, 2.651, 2.626, 2.609, 2.621, 2.652, 0.0]
[  349] loss:, [2.837, 2.82, 2.757, 2.758, 2.672, 2.666, 2.658, 2.633, 2.651, 2.621, 2.605, 2.619, 2.652, 0.0]
[  374] loss:, [2.837, 2.819, 2.756, 2.757, 2.668, 2.662, 2.654, 2.628, 2.647, 2.615, 2.598, 2.614, 2.645, 0.0]
[  399] loss:, [2.828, 2.809, 2.745, 2.746, 2.66, 2.653, 2.646, 2.62, 2.638, 2.606, 2.592, 2.605, 2.634, 0.0]
[  424] loss:, [2.824, 2.804, 2.746, 2.748, 2.665, 2.659, 2.65, 2.623, 2.644, 2.609, 2.597, 2.608, 2.637, 0.0]
[  449] loss:, [2.82, 2.801, 2.742, 2.745, 2.661, 2.654, 2.647, 2.62, 2.641, 2.605, 2.592, 2.605, 2.632, 0.0]
[  474] loss:, [2.816, 2.795, 2.738, 2.741, 2.66, 2.655, 2.646, 2.619, 2.639, 2.603, 2.591, 2.602, 2.63, 0.0]
[  499] loss:, [2.82, 2.797, 2.737, 2.74, 2.656, 2.651, 2.643, 2.614, 2.634, 2.603, 2.589, 2.6, 2.627, 0.0]
[  524] loss:, [2.819, 2.794, 2.735, 2.737, 2.654, 2.649, 2.641, 2.609, 2.631, 2.598, 2.585, 2.596, 2.622, 0.0]
[  549] loss:, [2.821, 2.795, 2.737, 2.738, 2.654, 2.65, 2.641, 2.611, 2.631, 2.598, 2.585, 2.595, 2.621, 0.0]
[  574] loss:, [2.818, 2.792, 2.734, 2.735, 2.653, 2.647, 2.637, 2.607, 2.627, 2.593, 2.582, 2.593, 2.617, 0.0]
[  599] loss:, [2.816, 2.791, 2.732, 2.733, 2.648, 2.643, 2.634, 2.601, 2.623, 2.593, 2.579, 2.589, 2.615, 0.0]
[  624] loss:, [2.811, 2.786, 2.731, 2.732, 2.647, 2.641, 2.635, 2.601, 2.624, 2.593, 2.58, 2.589, 2.615, 0.0]
[  649] loss:, [2.81, 2.783, 2.729, 2.729, 2.645, 2.641, 2.634, 2.602, 2.623, 2.59, 2.579, 2.589, 2.614, 0.0]
[  674] loss:, [2.811, 2.786, 2.731, 2.731, 2.647, 2.643, 2.635, 2.601, 2.622, 2.591, 2.579, 2.589, 2.615, 0.0]
[  699] loss:, [2.809, 2.785, 2.731, 2.731, 2.646, 2.641, 2.633, 2.601, 2.621, 2.59, 2.579, 2.588, 2.613, 0.0]
[  724] loss:, [2.809, 2.784, 2.731, 2.731, 2.647, 2.642, 2.634, 2.603, 2.621, 2.589, 2.577, 2.586, 2.613, 0.0]
[  749] loss:, [2.809, 2.784, 2.729, 2.729, 2.645, 2.64, 2.632, 2.602, 2.62, 2.587, 2.577, 2.585, 2.61, 0.0]
[  774] loss:, [2.811, 2.784, 2.729, 2.73, 2.647, 2.643, 2.635, 2.604, 2.623, 2.592, 2.58, 2.588, 2.615, 0.0]
[  799] loss:, [2.811, 2.783, 2.727, 2.727, 2.643, 2.639, 2.63, 2.6, 2.618, 2.588, 2.575, 2.583, 2.61, 0.0]
[  824] loss:, [2.813, 2.786, 2.73, 2.73, 2.647, 2.643, 2.633, 2.604, 2.622, 2.591, 2.578, 2.587, 2.612, 0.0]
[  849] loss:, [2.811, 2.785, 2.728, 2.728, 2.645, 2.64, 2.63, 2.602, 2.62, 2.587, 2.576, 2.584, 2.607, 0.0]
[  874] loss:, [2.811, 2.782, 2.726, 2.727, 2.644, 2.639, 2.629, 2.601, 2.619, 2.587, 2.574, 2.584, 2.608, 0.0]
[  899] loss:, [2.813, 2.783, 2.727, 2.728, 2.646, 2.642, 2.632, 2.604, 2.622, 2.591, 2.578, 2.587, 2.612, 0.0]
[  924] loss:, [2.816, 2.787, 2.73, 2.731, 2.648, 2.645, 2.636, 2.607, 2.624, 2.594, 2.581, 2.589, 2.616, 0.0]
[  949] loss:, [2.813, 2.786, 2.729, 2.731, 2.649, 2.645, 2.636, 2.608, 2.625, 2.593, 2.581, 2.589, 2.616, 0.0]
[  974] loss:, [2.813, 2.785, 2.729, 2.73, 2.647, 2.644, 2.635, 2.608, 2.624, 2.592, 2.579, 2.587, 2.614, 0.0]
[  999] loss:, [2.815, 2.788, 2.73, 2.733, 2.65, 2.647, 2.638, 2.609, 2.626, 2.594, 2.58, 2.587, 2.615, 0.0]
[ 1024] loss:, [2.816, 2.788, 2.73, 2.733, 2.65, 2.647, 2.638, 2.609, 2.625, 2.593, 2.58, 2.587, 2.614, 0.0]
[ 1049] loss:, [2.82, 2.791, 2.732, 2.735, 2.653, 2.65, 2.641, 2.613, 2.627, 2.597, 2.583, 2.59, 2.619, 0.0]
[ 1074] loss:, [2.818, 2.789, 2.731, 2.734, 2.65, 2.647, 2.638, 2.611, 2.623, 2.593, 2.58, 2.587, 2.615, 0.0]
[ 1099] loss:, [2.817, 2.788, 2.73, 2.732, 2.649, 2.646, 2.636, 2.61, 2.621, 2.592, 2.579, 2.585, 2.614, 0.0]
[ 1124] loss:, [2.818, 2.789, 2.729, 2.731, 2.649, 2.645, 2.636, 2.609, 2.62, 2.591, 2.577, 2.583, 2.613, 0.0]
[ 1149] loss:, [2.816, 2.787, 2.728, 2.73, 2.646, 2.643, 2.633, 2.606, 2.616, 2.588, 2.574, 2.58, 2.61, 0.0]
[ 1174] loss:, [2.817, 2.786, 2.727, 2.729, 2.645, 2.642, 2.632, 2.604, 2.614, 2.586, 2.573, 2.579, 2.609, 0.0]
[ 1199] loss:, [2.817, 2.786, 2.728, 2.729, 2.645, 2.642, 2.633, 2.605, 2.615, 2.587, 2.574, 2.579, 2.609, 0.0]
[ 1224] loss:, [2.816, 2.785, 2.727, 2.728, 2.645, 2.641, 2.632, 2.604, 2.614, 2.586, 2.573, 2.579, 2.607, 0.0]
[ 1249] loss:, [2.815, 2.783, 2.725, 2.727, 2.643, 2.64, 2.63, 2.603, 2.613, 2.585, 2.572, 2.578, 2.607, 0.0]
[ 1274] loss:, [2.815, 2.784, 2.726, 2.727, 2.644, 2.641, 2.631, 2.604, 2.615, 2.586, 2.574, 2.579, 2.608, 0.0]
[ 1299] loss:, [2.815, 2.784, 2.727, 2.729, 2.645, 2.642, 2.632, 2.605, 2.616, 2.587, 2.575, 2.58, 2.608, 0.0]
[ 1324] loss:, [2.814, 2.784, 2.727, 2.729, 2.646, 2.643, 2.633, 2.606, 2.616, 2.587, 2.576, 2.581, 2.609, 0.0]
[ 1349] loss:, [2.816, 2.785, 2.729, 2.73, 2.647, 2.643, 2.634, 2.607, 2.618, 2.588, 2.576, 2.581, 2.61, 0.0]
training finished

Process finished with exit code 0
